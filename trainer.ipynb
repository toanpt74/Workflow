{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toanpt74/Workflow/blob/main/trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "from job_env import job_shop_env\n",
        "from agent import Agent, ActorCritic\n",
        "from utils import v_wrap, plot_learning_curve\n",
        "import csv\n",
        "\n",
        "\n",
        "def train1(args):\n",
        "    torch.manual_seed(args.seed)\n",
        "\n",
        "    env = job_shop_env()\n",
        "\n",
        "    model = ActorCritic(env.state_dim, env.action_dim)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    state = env.reset()\n",
        "    state = v_wrap(state)\n",
        "    done = True\n",
        "    action_dim = env.expert\n",
        "\n",
        "    figure_file = './jss_a2c.png'\n",
        "\n",
        "    episode_length = 0\n",
        "    t_total_time = 0\n",
        "    complete_jobs = []\n",
        "    expert_complete_job = []\n",
        "    complete_job_start_time = []\n",
        "    update_list = []\n",
        "    score_history = []\n",
        "    for episode in range(args.episode):\n",
        "        if done:\n",
        "            cx = torch.zeros(1, 256)\n",
        "            hx = torch.zeros(1, 256)\n",
        "        else:\n",
        "            cx = cx.detach()\n",
        "            hx = hx.detach()\n",
        "        if len(complete_jobs) != 0:\n",
        "            update_list = [n for m in complete_jobs for n in m]\n",
        "            env.update(update_list)\n",
        "\n",
        "        values = []\n",
        "        log_probs = []\n",
        "        rewards = []\n",
        "        entropies = []\n",
        "\n",
        "        for step in range(args.num_steps + 1):\n",
        "            episode_length += 1\n",
        "\n",
        "            action, log_prob, entropy, value = model.choose_action((state, (hx, cx)), action_dim)\n",
        "\n",
        "            log_prob = log_prob.gather(1, action)[0]\n",
        "\n",
        "            state, reward, done, done_job, done_expert, job_start_time, total_time = env.step(action.view(-1, ).numpy())\n",
        "            t_total_time += total_time\n",
        "\n",
        "            done = done or episode_length >= args.max_episode_length\n",
        "            ## reward shaping\n",
        "            reward = max(min(reward, 1), -1)\n",
        "            if episode_length % 20 == 0:\n",
        "                print('reward: ', reward, 'total time: ', total_time)\n",
        "                score_history.append(reward)\n",
        "                # print(done_job)\n",
        "\n",
        "            if done:\n",
        "                complete_jobs.append(done_job)\n",
        "                expert_complete_job.append(done_expert)\n",
        "                complete_job_start_time.append(job_start_time)\n",
        "                print('Current episode:', episode)\n",
        "                episode_length = 0\n",
        "                state = env.reset()\n",
        "\n",
        "            state = v_wrap(state)\n",
        "            values.append(value)\n",
        "            log_probs.append(log_prob)\n",
        "            rewards.append(reward)\n",
        "            entropies.append(entropy)\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        if len(list(set(update_list))) > 8800:\n",
        "            ## write results into the csv file\n",
        "            with open('submit_{}.csv'.format(len(list(set(update_list)))), 'w') as f:\n",
        "                writer = csv.writer(f)\n",
        "                for i in range(len(complete_jobs)):\n",
        "                    for j in range(len(complete_jobs[i])):\n",
        "                        writer.writerow(\n",
        "                            [complete_jobs[i][j] + 1, expert_complete_job[i][j] + 1, complete_job_start_time[i][j]])\n",
        "\n",
        "        if episode == args.episode - 1 or len(list(set(update_list))) == 8840:\n",
        "            model.save_checkpoint()\n",
        "            ## write results into the csv file\n",
        "            with open('submit.csv', 'w') as f:\n",
        "                writer = csv.writer(f)\n",
        "                for i in range(len(complete_jobs)):\n",
        "                    for j in range(len(complete_jobs[i])):\n",
        "                        writer.writerow(\n",
        "                            [complete_jobs[i][j] + 1, expert_complete_job[i][j] + 1, complete_job_start_time[i][j]])\n",
        "            break\n",
        "\n",
        "        R = torch.zeros(1, 1)\n",
        "        if not done:\n",
        "            value, _, _ = model((state.unsqueeze(0), (hx, cx)))\n",
        "            R = value.detach()\n",
        "\n",
        "        values.append(R)\n",
        "        policy_loss = 0\n",
        "        value_loss = 0\n",
        "        gae = torch.zeros(1, 1)\n",
        "        for i in reversed(range(len(rewards))):\n",
        "            R = args.gamma * R + rewards[i]\n",
        "            advantage = R - values[i]\n",
        "            value_loss = value_loss + 0.5 * advantage.pow(2)\n",
        "\n",
        "            # Generalized Advantage Estimation\n",
        "            delta_t = rewards[i] + args.gamma * \\\n",
        "                      values[i + 1] - values[i]\n",
        "            gae = gae * args.gamma * args.gae_lambda + delta_t\n",
        "\n",
        "            policy_loss = policy_loss - \\\n",
        "                          log_probs[i] * gae.detach() - args.entropy_coef * entropies[i]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        (policy_loss + args.value_loss_coef * value_loss).backward(torch.ones_like(policy_loss))\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "\n",
        "        optimizer.step()\n",
        "        # print(policy_loss.mean() + args.value_loss_coef * value_loss)\n",
        "        print('para updated')\n",
        "    x = [i + 1 for i in range(len(score_history))]\n",
        "    plot_learning_curve(x, score_history, figure_file)\n",
        "\n",
        "# score_history = []\n",
        "def train(args):\n",
        "\n",
        "    torch.manual_seed(args.seed)\n",
        "\n",
        "    torch.cuda.set_device(0)\n",
        "\n",
        "    env = job_shop_env()\n",
        "\n",
        "    batch_size = 2\n",
        "    n_epochs = 4\n",
        "    alpha = 0.0003\n",
        "    beta = 0.001\n",
        "    model = Agent(n_actions=env.action_dim, batch_size=batch_size,\n",
        "                    alpha=alpha, n_epochs=n_epochs,\n",
        "                    input_dims=env.state_dim, beta=beta)\n",
        "\n",
        "    figure_file = './jss_ppo.png'\n",
        "\n",
        "\n",
        "    state = env.reset()\n",
        "    done = True\n",
        "    action_dim = env.expert\n",
        "\n",
        "    episode_length = 0\n",
        "    t_reward = 0.0\n",
        "    total_time = 0\n",
        "\n",
        "    complete_jobs = []\n",
        "    expert_complete_job = []\n",
        "    complete_job_start_time = []\n",
        "    update_list = []\n",
        "    score_history = []\n",
        "    time_history = []\n",
        "    values = []\n",
        "    log_probs = []\n",
        "    rewards = []\n",
        "    for episode in range(args.episode):\n",
        "        state = env.reset()\n",
        "        state = v_wrap(state)\n",
        "        if len(complete_jobs) != 0:\n",
        "            update_list = [n for m in complete_jobs for n in m]\n",
        "            env.update(update_list)\n",
        "\n",
        "        for step in range(args.num_steps + 1):\n",
        "\n",
        "            episode_length += 1\n",
        "\n",
        "            action, prob, value = model.choose_action(state, action_dim)\n",
        "            action = action.to('cuda')\n",
        "            prob = prob.gather(1, action)[0]\n",
        "\n",
        "\n",
        "\n",
        "            state, reward, done, done_job, done_expert, job_start_time, total_time = env.step(action.cpu().view(-1,).numpy())\n",
        "\n",
        "            done = done or episode_length >= args.max_episode_length\n",
        "            ## reward shaping\n",
        "            t_reward = max(min(reward, 1), -1)\n",
        "            total_time += total_time\n",
        "            model.remember(state, action, prob, value, t_reward, done)\n",
        "\n",
        "            if episode_length % 25 == 0:\n",
        "                model.learn()\n",
        "                print('para update-', t_reward,'total time-', total_time)\n",
        "                score_history.append(t_reward)\n",
        "                time_history.append(total_time)\n",
        "\n",
        "                complete_jobs.append(done_job)\n",
        "                expert_complete_job.append(done_expert)\n",
        "                complete_job_start_time.append(job_start_time)\n",
        "                print('Complete these jobs with 100 iterations:')\n",
        "                print(complete_jobs)\n",
        "\n",
        "            values.append(value)\n",
        "            log_probs.append(prob)\n",
        "            rewards.append(t_reward)\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        if len(list(set(update_list))) > 8800:\n",
        "            ## write results into the csv file\n",
        "            with open('submit_{}.csv'.format(len(list(set(update_list)))), 'w') as f:\n",
        "                writer = csv.writer(f)\n",
        "                for i in range(len(complete_jobs)):\n",
        "                    for j in range(len(complete_jobs[i])):\n",
        "                        writer.writerow(\n",
        "                            [complete_jobs[i][j] + 1, expert_complete_job[i][j] + 1, complete_job_start_time[i][j]])\n",
        "\n",
        "        if episode == args.episode - 1 or len(list(set(update_list))) == 8840:\n",
        "            ## write results into the csv file\n",
        "            with open('submit_ppo.csv', 'w') as f:\n",
        "                writer = csv.writer(f)\n",
        "                for i in range(len(complete_jobs)):\n",
        "                    for j in range(len(complete_jobs[i])):\n",
        "                        writer.writerow(\n",
        "                            [complete_jobs[i][j] + 1, expert_complete_job[i][j] + 1, complete_job_start_time[i][j]])\n",
        "            break\n",
        "\n",
        "        if t_reward >= 0.55 or episode >= 148:\n",
        "            model.save_models()\n",
        "        print('episode:', episode, 'reward:', t_reward, 'total time:', total_time)\n",
        "\n",
        "    x = [i + 1 for i in range(len(score_history))]\n",
        "    plot_learning_curve(x, score_history, figure_file)\n",
        "\n",
        "# print(score_history)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WQauE9GCH-xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KOp8XRUFH-1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LcX8YVj-H-4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FdVNh-2yH-8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KgpgWYqZH-_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eUhX03GwH_C8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qii85UEbH_GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8oDiYWieH_Ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3w_4cM0BH_Mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qXa5pLV5H_Pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cW45cY7mH_TO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}