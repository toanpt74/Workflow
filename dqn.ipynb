{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toanpt74/Workflow/blob/main/dqn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "# import gym\n",
        "\n",
        "class replay_buffer(object):\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = deque(maxlen=self.capacity)\n",
        "\n",
        "    def store(self, observation, action, reward, next_observation, done, ):\n",
        "        observation = np.expand_dims(observation, 0)\n",
        "        next_observation = np.expand_dims(next_observation, 0)\n",
        "        self.memory.append([observation, action, reward, next_observation, done])\n",
        "\n",
        "    def sample(self, size):\n",
        "        batch = random.sample(self.memory, size)\n",
        "        observation, action, reward, next_observation, done = zip(* batch)\n",
        "        return np.concatenate(observation, 0), action, reward, np.concatenate(next_observation, 0), done\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "class dqn(nn.Module):\n",
        "    def __init__(self, observation_dim, action_dim):\n",
        "        super(dqn, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(self.observation_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, self.action_dim)\n",
        "\n",
        "    def forward(self, observation):\n",
        "        x = self.fc1(observation)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def get_action(self, observation, epsilon, action_dim):\n",
        "        action = torch.zeros(action_dim, action_dim, dtype=torch.int64)\n",
        "        if random.random() > epsilon:\n",
        "            q_value = self.forward(observation)\n",
        "            action_index = q_value.max(1)[1].item()\n",
        "            # action = q_value.max(1)[1].data[0].item()\n",
        "            action[:, action_index] = 1\n",
        "        else:\n",
        "            action = random.choice(list(range(self.action_dim)))\n",
        "        return action\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def training(buffer, batch_size, model, optimizer, gamma, loss_fn):\n",
        "    observation, action, reward, next_observation, done = buffer.sample(batch_size)\n",
        "\n",
        "    observation = torch.FloatTensor(observation)\n",
        "    action = torch.LongTensor(action)\n",
        "    reward = torch.FloatTensor(reward)\n",
        "    next_observation = torch.FloatTensor(next_observation)\n",
        "    done = torch.FloatTensor(done)\n",
        "\n",
        "    q_values = model.forward(observation)\n",
        "    next_q_values = model.forward(next_observation)\n",
        "\n",
        "    q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
        "    next_q_value = next_q_values.max(1)[0].detach()\n",
        "    expected_q_value = reward + next_q_value * (1 - done) * gamma\n",
        "\n",
        "    loss = loss_fn(q_value, expected_q_value.detach())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     epsilon_init = 0.9\n",
        "#     epsilon_min = 0.01\n",
        "#     decay = 0.995\n",
        "#     capacity = 10000\n",
        "#     exploration = 5000\n",
        "#     batch_size = 64\n",
        "#     episode = 1000000\n",
        "#     render = True\n",
        "#     learning_rate = 1e-3\n",
        "#     gamma = 0.99\n",
        "#     loss_fn = nn.MSELoss()\n",
        "#\n",
        "#     env = gym.make('CartPole-v0')\n",
        "#     env = env.unwrapped\n",
        "#     action_dim = env.action_space.n\n",
        "#     observation_dim = env.observation_space.shape[0]\n",
        "#\n",
        "#     model = dqn(observation_dim, action_dim)\n",
        "#     optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
        "#     buffer = replay_buffer(capacity)\n",
        "#     epsilon = epsilon_init\n",
        "#     weight_reward = None\n",
        "#\n",
        "#     for i in range(episode):\n",
        "#         obs = env.reset()\n",
        "#         if epsilon > epsilon_min:\n",
        "#             epsilon = epsilon * decay\n",
        "#         reward_total = 0\n",
        "#         if render:\n",
        "#             env.render()\n",
        "#         while True:\n",
        "#             action = model.get_action(torch.FloatTensor(np.expand_dims(obs, 0)), epsilon)\n",
        "#             train_flag = False\n",
        "#             next_obs, reward, done, info = env.step(action)\n",
        "#             if render:\n",
        "#                 env.render()\n",
        "#             buffer.store(obs, action, reward, next_obs, done)\n",
        "#             reward_total += reward\n",
        "#             obs = next_obs\n",
        "#             if len(buffer) > exploration:\n",
        "#                 training(buffer, batch_size, model, optimizer, gamma, loss_fn)\n",
        "#                 train_flag = True\n",
        "#             if done:\n",
        "#                 if not weight_reward:\n",
        "#                     weight_reward = reward_total\n",
        "#                 else:\n",
        "#                     weight_reward = 0.99 * weight_reward + 0.01 * reward_total\n",
        "#                 print('episode: {}  reward: {}  epsilon: {:.2f}  train:  {}  weight_reward: {:.3f}'.format(i+1, reward_total, epsilon, train_flag, weight_reward))\n",
        "#                 break"
      ],
      "metadata": {
        "id": "WQauE9GCH-xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M2QQtLguMaYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KOp8XRUFH-1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LcX8YVj-H-4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FdVNh-2yH-8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KgpgWYqZH-_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eUhX03GwH_C8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qii85UEbH_GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8oDiYWieH_Ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3w_4cM0BH_Mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qXa5pLV5H_Pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cW45cY7mH_TO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}