{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toanpt74/Workflow/blob/main/train_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from job_env import job_shop_env\n",
        "from Baseline_Q.dqn import dqn, replay_buffer\n",
        "import torch.optim as optim\n",
        "from utils import v_wrap\n",
        "import torch.nn as nn\n",
        "\n",
        "def training(buffer, batch_size, model, optimizer, gamma, loss_fn):\n",
        "    observation, action, reward, next_observation, done = buffer.sample(batch_size)\n",
        "\n",
        "    observation = torch.FloatTensor(observation)\n",
        "    action = torch.LongTensor(action)\n",
        "    reward = torch.FloatTensor(reward)\n",
        "    next_observation = torch.FloatTensor(next_observation)\n",
        "    done = torch.FloatTensor(done)\n",
        "\n",
        "    q_values = model.forward(observation)\n",
        "    next_q_values = model.forward(next_observation)\n",
        "\n",
        "    q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
        "    next_q_value = next_q_values.max(1)[0].detach()\n",
        "    expected_q_value = reward + next_q_value * (1 - done) * gamma\n",
        "\n",
        "    loss = loss_fn(q_value, expected_q_value.detach())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "def training_baseline(args):\n",
        "    torch.manual_seed(args.seed)\n",
        "\n",
        "    env = job_shop_env()\n",
        "\n",
        "    model = dqn(env.state_dim, env.action_dim)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "    model.train()\n",
        "\n",
        "    state = env.reset()\n",
        "    state = v_wrap(state)\n",
        "    done = True\n",
        "    action_dim = env.expert\n",
        "\n",
        "    epsilon_init = 0.9\n",
        "    capacity = 100000\n",
        "    exploration = 100000\n",
        "    buffer = replay_buffer(capacity)\n",
        "    epsilon = epsilon_init\n",
        "    weight_reward = None\n",
        "    episode_length = 0\n",
        "    complete_jobs = []\n",
        "    expert_complete_job = []\n",
        "    complete_job_start_time = []\n",
        "    update_list = []\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    for episode in range(args.episode):\n",
        "        obs = env.reset()\n",
        "        # state = v_wrap(state)\n",
        "        if len(complete_jobs) != 0:\n",
        "            update_list = [n for m in complete_jobs for n in m]\n",
        "            env.update(update_list)\n",
        "\n",
        "        reward_total = 0\n",
        "\n",
        "        for step in range(args.num_steps+1):\n",
        "            episode_length += 1\n",
        "\n",
        "            action = model.get_action()\n",
        "            next_obs, reward, done, done_job, done_expert, job_start_time = env.step(action.view(-1,).numpy())\n",
        "            done = done or episode_length >= args.max_episode_length\n",
        "            buffer.store(obs, action, reward, next_obs, done)\n",
        "            reward_total += reward\n",
        "            obs = next_obs\n",
        "\n",
        "            if len(buffer) > exploration:\n",
        "                training(buffer=buffer,batch_size=16, model=model, optimizer=optimizer, gamma=args.gamma, loss_fn=loss_fn)\n",
        "            if done:\n",
        "                if not weight_reward:\n",
        "                    weight_reward = reward_total\n",
        "                else:\n",
        "                    weight_reward = 0.99 * weight_reward + 0.01 * reward_total\n",
        "                print('episode: {} reward: {} epsilon: {:.5f} weight_reward: {:.5f}').format(step+1, reward_total, epsilon, weight_reward)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WQauE9GCH-xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M2QQtLguMaYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KOp8XRUFH-1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LcX8YVj-H-4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FdVNh-2yH-8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KgpgWYqZH-_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eUhX03GwH_C8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qii85UEbH_GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8oDiYWieH_Ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3w_4cM0BH_Mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qXa5pLV5H_Pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cW45cY7mH_TO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}