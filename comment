///////////////////////////////////////////////////////////////////
Tạo chương trình test cho các luồng công việc cụ thể
from test_funcs.compute_greedy_makespan import greedy_average_makespan
from test_funcs.sample_average_makespan import sample_average_makespan
from My_dataset_info import *
from MyDataloader import MyDataloader
from rcpsp_simulator.skip_env import Skip_environment
import numpy as np
import time

import os
import random
import time
from Params import args
from model.actor_critic import Agent
from rcpsp_simulator.normal_env import Normal_environment
from rcpsp_simulator.skip_env import Skip_environment

from MyDataloader import MyDataloader
from My_dataset_info import  *
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions.categorical import Categorical
from torch.utils.tensorboard import SummaryWriter
import copy
from test_funcs.compute_greedy_makespan import greedy_average_makespan
from tqdm import tqdm
model_30 = r"D:\RCPSP\RL-RCPSP\saves\set_301160_321736313092.0018058.pt"

model = r"D:\RCPSP\RL-RCPSP\saves\set_301160_321736313092.0018058.pt"


dataloader_test_30 = MyDataloader(test_set_30)


n_samples = 30
greedy_test = False
normal_env = False
if_variant = False


## normal test ###
all_model = [model_30]
all_dataloader = [dataloader_test_30]



for model, dataloader in zip(all_model, all_dataloader):
    print(model)
    average_makespan, average_time = sample_average_makespan(model, dataloader, n_sampels=n_samples,
                                                             normal_env=normal_env, variant=if_variant,
                                                             greedy_test=greedy_test)
    print(average_makespan, average_time)
#Để lây thông tin về makespan của toàn bộ các job trong chương trình, thêm lệnh print(all_data_store) vào cuối tệp tin sample_average_makespan

///////////////////////////////////////////////////////////////////
feature_mat: là ma trận lưu trữ các thông tin về duration và resource của các job

///////////////////////////////////////////////////////////////////
1. Tổ chức data trong các tệp tin job
+ job    mode    #sucsessor    sucsessors : lưu trữ số hiệu job, mode, số nút con, danh sách nút con
    1    1        3            2 3 4
    2    1        3            6 11 15
    3    1        3            7 8 13
+ Chú ý: job thực sự chỉ tính từ hàng 1 đến hàng 31. 

2. Tạo jobdag từ tệp tin với các thông tin ma trận feature, ma trận kề, tài nguyên
+ Sau khi đọc xong dữ liệu từ tệp tin, thực hiện xóa hai hàng đầu và hàng cuối
+ XÓa hai cột đầu và cột cuối 
+ Có ma trận kề gồm 30 hàng, 30 cột
+ Mỗi nút trong đồ thị gồm các thông tin chính: new_node = Node(idx, task_duration, resource1, resource2, resource3, resource4)
    - idx (số thứ tự node): đánh số từ 0
    - resource của node: gồm 4 resource là resource_1,resource_2,resource_3,resource_4,
    - duration
+ Sử dụng list là nodes để chứa toàn bộ các nút của đồ thị (gồm 30 nút, từ nút 0 đến nút 29)
+ Mỗi nút ứng với một hàng trong ma trận feature của tệp tin job.sm (từ hàng 1 đến hàng 31)

///////////////////////////////////////////////////////////////////
Không gian trạng thái state được biểu diễn bởi các đặc trưng cơ bản:
+ jobdag: là đồ thị luồng công việc (jobdag = Jobdag(adj_mat, nodes_information, resource): tạo một đồ thị với ma trận kề, ma trận thuộc tính (chính là ma trận duration của các job trên các resource) )

+ Lệnh  env = Normal_environment() tạo ra một đối tượng env gồm các thuộc tính:
- self.tep = namedtuple('state', ['adj_mat', 'feature_mat', 'resource_exec','runable_nodes_idx', 'action_mask'])
- ma trận kề: adj_mat được gán là none
- last_decision_time: thời gian và gán bằng 0.
+ Lệnh ini_state = env.reset(adj, fea, resource, variant) gọi thủ tục reset trong lớp Normal_environment để khởi tạo các giá trị cho biến môi trường env
với các tham số là ma trận kề, ma trận đặc trưng feature, ma trận tài nguyên resource và ma trận variant.
 - Tạo ra đồ thị luồng công việc jobdag (Ghi chú: đồ thị luồng công việc được tạo ngẫu nhiên với các nút cha con)
 - Tạo bộ thực thi executor = Normal_executor(jobdag, resource_variant). Bộ executor gồm các thuộc tính cơ bản:
                           -  đồ thị luồng công việc jobdag
                           - ma trận kề adj_mat
                           - ma trận đặc trưng fea_mat
                           - ma trận tài nguyên resource_execute
                           - thời gian walltime = 0
                           - một số thuộc tính khác  
- Gán thuộc tính: last_decision_time = 0
- Kết quả trả về của thủ tục reset() là state (state = self.tep(self.adj_mat, self.executor.feature_mat, self.executor.resource_exec, self.executor.runable_nodes_idx, self.executor.now_action_mask))

+ list all_ini_states lưu trữ tất cả các trạng thái khởi tạo (gồm 8 state):  state = self.tep(self.adj_mat, self.executor.feature_mat, self.executor.resource_exec, self.executor.runable_nodes_idx, self.executor.now_action_mask)
+ biến all_next_obs = all_ini_states
+ biến all_ini_states, all_next_obs là một list gồm 8 phần tử, mỗi phần tử chưa một state
+ biến obs là một list gồm 32 phần tử  = num_steps. Mỗi phần tử của obs chứa ma trận all_next_obs / all_ini_states. Lệnh gán obs[step] = copy.deepcopy(all_next_obs) 
+ all_next_obs là một mảng chứa 8 trạng thái state
+ Thủ tục: get_action_and_value(self,
                state,
                action = None,
                greedy_test = False
                ):
    - Thông tin truyền vào là trạng thái state
    - Đầu ra là: action, log_prob, probs.entropy(), value
    - Xử lý: lấy các thông tin state[0]: ma trận kề, state[1] là ma trận đặc trưng (duration and resource), state[2] là ma trận tài nguyên
    - runable_nodes_idx: chỉ số các nút thực thi (ngầm định là 0, 1, 2) = 3 nút đầu tiên
+ actions: là một tensor gồm 32 hàng, mỗi hàng là 8 cột, lưu các chỉ số node (0,1, 2,...)
+ Vòng lặp thứ 2:  for env_idx in range(args.num_envs):
- Duyệt từ 0 đến 8. env_idx = 0, 1, 2, ...,8
- Lấy action từ tensor actions: a = copy.deepcopy(actions[step][env_idx].cpu().numpy())
- Tính các đại lượng next_obs, reward, done = envs[env_idx].step(a)
+ Phương thức step() trong lớp: Normal_environment


+ ma trận kề adjacency matrix
+ ma trận thuộc tính feature matrix
+ ma trận tài nguyên Resource excute
+ runable node index: một list VD: [0, 1, 2]
+ ma trận action_mask = có số chiều bằng số job, đây là ma trận nhị phân biểu diễn số trạng thái job được thực thi
 action_mask =[1, 1, 1, 0, 0, ...0] (khởi tạo có 3 job được thực thi là job 0, 1, 2)

+ action = 0, 1, 2,....,30.
//////////////////////////////////////////////////////////
Ví dụ: 
Bước 1: Chọn nút 2 thực thi
=> Gọi thủ tục step(action=2)
+ Kiểm tra các nút cha của nút 2 xem đã hoàn thành chưa. Do nút 2 không có nút cha
+ Gọi thủ tục assign_task(action=2) để gán nút 2 vào thực thi
=> action_sequence =[2]
và running_tasks =[2]
+ Loại bỏ nút 2 khỏi danh sách runable_nodes_idx
+ Giảm tài nguyên tổng thể
+ Tìm các nút con của nút 2, kiểm tra điều kiện để gán vào danh sách runable_nodes_idx => runable_nodes_idx=[0, 1, 3, 7, 8]
Bước 2: Chọn nút 8 để thực thi
=> Gọi thủ tục step(action=8)
+ Kiểm tra nút cha của nút 8 xem đã hoàn thành chưa? (nút 2)
+ Đợi cho đến khi nút 2 hoàn thành => walltime = 6.
+ Gọi thủ tục assign_task(action=8) để gán nút 8 vào thực thi
=> action_sequence =[2, 8]
và running_tasks =[8]
+ Loại bỏ nút 8 khỏi danh sách runable_nodes_idx
+ Giảm tài nguyên tổng thể
+ Tìm các nút con của nút 8, kiểm tra điều kiện để gán vào danh sách runable_nodes_idx => runable_nodes_idx=[0, 1, 3, 7, 14]
Bước 3: Chọn nút 7 để thực thi
=> Gọi thủ tục step(action=7)
+ Kiểm tra các nút cha của nút 7 xem đã hoàn thành chưa.
=> Nút cha của 7 là 2 đã hoàn thành, do vậy last_decision_time = 6.
+ Gọi thủ tục assign_task(action=7) để gán nút 7 vào thực thi
=> action_sequence =[2, 8, 7]
và running_tasks =[8, 7]
+ Loại bỏ nút 7 khỏi danh sách runable_nodes_idx
+ Giảm tài nguyên tổng thể
+ Tìm các nút con của nút 7, kiểm tra điều kiện để gán vào danh sách runable_nodes_idx => runable_nodes_idx=[0, 1, 3, 14]
Bước 4: chọn action = 3
=> Gọi thủ tục step(action=3)
+ Kiểm tra các nút cha của nút 3 xem đã hoàn thành chưa.
=> Nút cha của 3 là 2 đã hoàn thành, do vậy last_decision_time = 6.
+ Gọi thủ tục assign_task(action=3) để gán nút 3 vào thực thi
=> action_sequence =[2, 8, 7, 3]
và running_tasks =[8, 7, 3]
+ Loại bỏ nút 3 khỏi danh sách runable_nodes_idx
+ Giảm tài nguyên tổng thể
+ Tìm các nút con của nút 3, kiểm tra điều kiện để gán vào danh sách runable_nodes_idx => runable_nodes_idx=[0, 1, 14]
Bước 5: chọn action = 14
=> Gọi thủ tục step(action=14)
+ Kiểm tra các nút cha của nút 14 xem đã hoàn thành chưa (nút 14 có cha là nút 8).
=> Đợi cho đến khi nút 8 hoàn thành (nút 8 có duration = 7 => thời gian walltime = 6+7 =13, và last_decision_time=13)
+ Đánh dấu các nút con của nút 8 (completed_parent_nodes[14] = 8, completed_parent_nodes[23] = 8)
+ walltime = 13
+ Gọi thủ tục assign_task(action=14) để gán nút 14 vào thực thi
=> action_sequence =[2, 8, 7, 3, 14]
và running_tasks =[14]
+ Loại bỏ nút 14 khỏi danh sách runable_nodes_idx
+ Giảm tài nguyên tổng thể
+ Tìm các nút con của nút 3, kiểm tra điều kiện để gán vào danh sách runable_nodes_idx => runable_nodes_idx=[0, 1, 19]
Bước 6: chọn nút 19
=> Gọi thủ tục step(action=19)
+ Kiểm tra các nút cha của nút 19 xem đã hoàn thành chưa (nút 19 có cha là nút 14).
=> Đợi cho đến khi nút 14 hoàn thành (nút 14 có duration = 10 => thời gian walltime = 6+7 +10 =23, và last_decision_time=23)
+ Đánh dấu các nút con của nút 14 (completed_parent_nodes[19] = 14, completed_parent_nodes[20] = 14)
+ walltime = 23
+ Gọi thủ tục assign_task(action=19) để gán nút 19 vào thực thi
=> action_sequence =[2, 8, 7, 3, 14,19]
và running_tasks =[19]
+ Loại bỏ nút 19 khỏi danh sách runable_nodes_idx
+ Giảm tài nguyên tổng thể
+ Tìm các nút con của nút 19, kiểm tra điều kiện để gán vào danh sách runable_nodes_idx => runable_nodes_idx=[0, 1]
Bước 7: chọn nút 1
=> Gọi thủ tục step(action=1)
+ Kiểm tra các nút cha của nút 1 xem đã hoàn thành chưa (nút 1 không có cha).
 
+ Đánh dấu các nút con của nút 1 (completed_parent_nodes[5] = 1, completed_parent_nodes[6] = 1,completed_parent_nodes[11] = 1)
+ walltime = 23
+ Gọi thủ tục assign_task(action=1) để gán nút 1 vào thực thi
=> action_sequence =[2, 8, 7, 3, 14,19,1]
và running_tasks =[19, 1]
+ Loại bỏ nút 1 khỏi danh sách runable_nodes_idx
+ Giảm tài nguyên tổng thể
+ Tìm các nút con của nút 1, kiểm tra điều kiện để gán vào danh sách runable_nodes_idx => runable_nodes_idx=[0, 5, 6, 11]
last_decision_time = 23

..............
Bước i:
+ Chọn action = 27
=> Gọi thủ tục step(action=27); runable_nodes_idx = [5, 11, 4, 13, 24, 12, 27]
+ Kiểm tra các nút cha của nút 27 xem đã hoàn thành chưa (nút 27 có cha là nút 17).
=> Đợi cho đến khi nút 17 hoàn thành (nút 17 có duration = 3 => thời gian walltime = 36 + 3 =39, và last_decision_time=39)
+ Đánh dấu các nút con của nút 17 (completed_parent_nodes[12] = 17)
+ walltime = 39
+ Gọi thủ tục assign_task(action=27) để gán nút 27 vào thực thi
=> action_sequence =[2, 8, 7, 3, 14, 19, 1, 6, 0, 9, 10, 17, 27]
và running_tasks =[9, 27]
+ Loại bỏ nút 27 khỏi danh sách runable_nodes_idx
+ Giảm tài nguyên tổng thể
+ Tìm các nút con của nút 27, kiểm tra điều kiện để gán vào danh sách runable_nodes_idx => runable_nodes_idx=[5, 11, 4, 13, 24, 12]
//=====================================================
Phương pháp thay đổi cách tính Reward
1. Kết hợp duration và resource, 
